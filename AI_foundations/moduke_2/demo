Machine Learning Use Case – Logistic Regression Demo
Machine Learning Process

✅ Steps in ML Process -->
--> Load the data
--> Preprocess the data
--> Train a model
--> Evaluate the model
--> Make predictions

About Classifiers

✅ Classifier --> Algorithm or model used for classification tasks
✅ Goal --> Assign a category or label to a given input based on its features
✅ Purpose --> Learn patterns and relationships in training data to predict labels for new unseen data

Demo Overview

✅ Libraries used -->
--> pandas for data manipulation (dataframes and series)
--> LogisticRegression from scikit-learn for building the classifier

✅ Installation (if libraries not available in kernel) -->

conda install -c anaconda scikit-learn

Loading the Dataset

✅ Dataset --> iris.csv containing different species of iris flowers
✅ Load dataset --> Use pandas read_csv method:

iris_data = pd.read_csv('iris.csv')


✅ Preview dataset --> Use .head() to inspect first five rows

Splitting Features and Labels

✅ Features (X) --> Attributes of iris flowers (sepal length, sepal width, petal length, petal width)
--> Drop ID and species columns
✅ Labels (y) --> Species of iris (target variable)

X = iris_data.drop(columns=['ID', 'species'])
y = iris_data['species']

Creating the Model

✅ Model Initialization -->

model = LogisticRegression()


--> Initializes a logistic regression classifier with default settings

Training the Model

✅ Training --> Learn relationship between features X and labels y

model.fit(X, y)

Making Predictions

✅ Prediction --> Use trained model on new/unseen data points

predictions = model.predict(new_data)
print(predictions)


✅ Output --> Predicted species of iris flowers (e.g., Iris-setosa)

Summary of Demo

✅ Loaded dataset (iris.csv)
✅ Split into features (X) and labels (y)
✅ Created logistic regression model
✅ Trained the model on the dataset
✅ Made predictions on unseen data
✅ Printed predictions to the console



DEMO PART 2---------------------->


ML Demo – Extended Workflow
Notebook Setup

✅ Duplicate existing notebook --> Rename as MLDemo2
✅ Close old notebook --> Shut down the previous tab
✅ Restart Kernel --> Clear all outputs for a clean start

Importing Libraries

✅ NumPy --> import numpy as np
--> Used for numerical computations, arrays, and math functions

✅ Train/Test Split --> from sklearn.model_selection import train_test_split
--> Splits dataset into training and testing sets

✅ StandardScaler --> from sklearn.preprocessing import StandardScaler
--> Standardizes features to mean = 0 and std = 1

✅ Accuracy Score --> from sklearn.metrics import accuracy_score
--> Calculates the accuracy of predictions

Standardization

✅ Purpose --> Ensure all features are on the same scale
✅ Example --> Predicting house prices with square footage (1000–5000) and number of bedrooms (1–6)
--> Without standardization, larger magnitude features dominate learning
✅ Standardization Formula -->

X_scaled = (X - mean) / standard_deviation

Train/Test Split

✅ Split features (X) and labels (y) --> Features X = flower attributes, Labels y = species
✅ Split data -->

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


--> random_state ensures reproducibility

Feature Scaling

✅ Create scaler instance -->

scaler = StandardScaler()


✅ Fit and transform training data --> Scale X_train
✅ Transform testing data --> Scale X_test

Model Creation & Training

✅ Initialize logistic regression model -->

model = LogisticRegression()


✅ Train model on scaled data -->

model.fit(X_train_scaled, y_train)


--> Model learns relationships between features and labels

Model Evaluation

✅ Predict on test set -->

y_pred = model.predict(X_test_scaled)


✅ Calculate accuracy -->

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)


--> Example output: 1.0 → 100% accurate on test set

Making Predictions on New Data

✅ Create new sample data --> NumPy array representing iris flower attributes

new_data = np.array([[5.1, 3.5, 1.4, 0.2],
                     [6.3, 3.3, 6.0, 2.5],
                     [5.0, 3.6, 1.4, 0.2]])


✅ Standardize new data --> Using the same scaler fitted on training data

✅ Predict species -->

predictions = model.predict(new_data_scaled)
print(predictions)


✅ Example Output -->
--> ['Iris-setosa', 'Iris-virginica', 'Iris-setosa']

Summary of Extended Demo

✅ Workflow Covered -->
--> Data loading
--> Feature and label separation
--> Train/test split
--> Standardization of features
--> Model creation (logistic regression)
--> Model training
--> Prediction on test data
--> Accuracy evaluation
--> Prediction on new unseen data

✅ Key Takeaways -->
--> Standardization prevents feature dominance
--> Train/test split ensures model generalization
--> Accuracy score validates performance
--> Using the trained model, predictions can be made on new data