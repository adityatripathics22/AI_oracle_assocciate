✅ Deep Learning Overview
--> Deep learning is a subset of Machine Learning
--> It focuses on training Artificial Neural Networks (ANNs) to learn from raw data
--> Key difference from ML
--> ML requires manual feature extraction (e.g., handwriting: loops, strokes)
--> Deep Learning automatically learns features (edges, shapes, structures)

Example: Handwritten digits → Different people write “2” differently, yet ANN can still recognize it by learning curves and slants automatically.

✅ Why Deep Learning?
--> Traditional ML struggles when features are hard to define
--> Deep learning can:
--> Process raw data (pixels, sound waves, words)
--> Extract complex patterns automatically
--> Use parallel computation (GPUs) to scale

Example:
--> ML approach: Manually define “a cat has ears, whiskers, tail”
--> Deep learning: Learns cat features automatically from thousands of cat photos

✅ History of Deep Learning
--> 1950s → Neuron & Perceptron concepts
--> 1980s → Backpropagation introduced
--> 1990s → CNNs for image tasks
--> 2000s → GPUs introduced
--> 2010s → Cheap GPUs → boom in AI for vision, NLP, speech
--> 2012 → AlexNet revolutionized image classification
--> 2016 onwards → Generative models (GANs, transformers)
--> Today → Used in LLMs (ChatGPT), image generators (DALL·E, Stable Diffusion)

✅ Applications of Deep Learning
--> Images → classification, object detection, segmentation, facial recognition
--> Text → translation, summarization, sentiment analysis, Q&A
--> Audio → speech-to-text, music generation, speech synthesis
--> Generative AI → text-to-image, chatbots, video generation

✅ Choosing the Right Architecture
--> CNNs → Image-related tasks
--> RNNs / LSTMs → Sequential data (text, time series, speech)
--> Transformers → Latest models for text and vision (e.g., GPT, ViT)
--> GANs / Diffusion Models → Generative tasks (art, image creation, style transfer)

✅ Artificial Neural Networks (ANNs)
--> Inspired by the human brain
--> Building blocks
--> Layers: Input, Hidden, Output
--> Neurons: Basic computational units
--> Weights: Connection strength
--> Bias: Flexibility term
--> Activation Functions: Decide if a neuron “fires” (ReLU, Sigmoid, etc.)

Example:
Input pixels → [0.1, 0.9, 0.2]
Weights → [0.5, 0.3, 0.7]
Weighted sum = (0.1×0.5 + 0.9×0.3 + 0.2×0.7)
Activation function → produces neuron output

✅ Example: Handwritten Digit Recognition (MNIST)
--> Dataset: 28×28 pixel images (784 inputs)
--> Input Layer: 784 neurons
--> Hidden Layers: 2 layers of 16 neurons each (learn edges, loops)
--> Output Layer: 10 neurons (digits 0–9)

Training Process (Backpropagation)
--> Show an image (say “2”)
--> Network predicts wrong (e.g., “6”)
--> Calculate error
--> Adjust weights using backpropagation
--> Repeat with thousands of images
--> Over time → ANN learns to classify digits correctly

✅ In Summary
--> Deep Learning = ML that learns features automatically
--> Handles large, complex data using GPUs
--> Powers modern AI in vision, text, audio, and generative tasks
--> ANN is built with Layers, Neurons, Weights, Bias, Activation
--> Backpropagation is key for training models